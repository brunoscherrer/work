\documentclass{article}
\usepackage{fullpage}
\usepackage[utf8x]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{autonum}
\usepackage{dsfont}
\usepackage{stmaryrd}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}


\title{A strongly polynomial algorithm for mean payoff games}

\author{Bruno Scherrer}
\def\R{\mathds R}
\def\1{{\mathds 1}}

\newcommand{\intset}[1]{\llbracket #1 \rrbracket}
\newcommand{\intpart}[1]{\lceil #1 \rceil}

\begin{document}
\maketitle

\begin{abstract}
...
\end{abstract}
  
We consider an infinite-horizon game on a directed graph $(X,E)$ between two players, MAX and MIN. For any vertex $x$, we write $E(x)=\{y;(x,y) \in E\}$ for the set of vertices that can be reached from $x$ by following one edge and we assume $E(x)\neq\emptyset$.  The set of vertices $X=\{1,2,\dots,n\}$ of the graph is partitionned into the sets $X_+$ and $X_-$ of nodes respectively controlled by MAX and MIN. The game starts in some vertex $x_0$. At each time step, the player who controls the current vertex chooses a next vertex by following an edge. So on and so forth, the choices generate an infinitely long trajectory $(x_0,x_1,\dots)$. We shall mainly consider the $\gamma$-discounted payoff for some $0\le\gamma<1$, where the goal of MAX is to maximize
\begin{align}
(1-\gamma)\sum_{t=0}^{\infty} \gamma^t r(x_t)
\end{align}
while that of MIN is to minimize this quantity.

LITERATURE

\section{Preliminaries}

Let $M$ and $N$ be the set of stationary policies for MAX and MIN:
\begin{align}
  M &= \{ \mu:X_+ \to X ~;~ \forall x\in X_+,~ \mu(x) \in E(x) \}, \\
  N & =\{ \nu:X_- \to X ~;~ \forall x\in X_-,~ \nu(x) \in E(x) \}.
\end{align}

For any policies $\mu \in M$ and $\nu \in N$, let us write $P_{\mu,\nu}$ for the transition matrix induced by $\mu$ and $\nu$:
\begin{align}
  \forall x \in X_+, \forall y \in X, ~~~P_{\mu,\nu}(x,y)=\1_{\mu(x)=y}, \\
  \forall x \in X_-, \forall y \in X, ~~~P_{\mu,\nu}(x,y)=\1_{\nu(x)=y}.
\end{align} 
Seeing the reward $r:X \to {0,1,\dots,R}$ and any function $v:X \to \R$ as vectors of $\R^n$, consider the following Bellman operators
\begin{align}
  T_{\mu,\nu}v & = (1-\gamma)r + \gamma P_{\mu,\nu}v, \\
  T v & =\max_\mu \min_\nu T_{\mu,\nu }v .
\end{align}
that are $\gamma$-contractions with respect to the max-norm $\|\cdot\|$, defined for all $u \in R^n$ as $\|u\|=\max_{x \in X} |u(x)|$.
For any policies $\mu \in M$ and $\nu \in N$, the value $v_{\mu,\nu}(x)$ obtained by following policies $\mu$ and $\nu$ satisfies
\begin{align}
v_{\mu,\nu} = (1-\gamma)\sum_{t=0}^\infty (\gamma P_{\mu,\nu})^t r = (1-\gamma)(I-\gamma P_{\mu,\nu})^{-1}r,
\end{align}
and is the only fixed point of the operator $T_{\mu,\nu}$.
The optimal value
\begin{align}
  v_* = \max_{\mu} \min_{\nu} v_{\mu,\nu}
\end{align}
is the fixed point of the operator $T$. Let $(\mu_*,\nu_*)$ be any pair of positional strategies such that $T_{\mu_*,\nu_*}v_*=T v_*$. It is well-known that $(\mu_*,\nu_*)$ is optimal.


\section{Algorithm}

Solve the $n$-step problem with terminal cost, i.e. identify a set of strategies $\mu_1,\dots,\mu_n$ and $\nu_1,\dots,\nu_n$ such that:
\begin{align}
  T^n 0 = T_{\mu_1,\nu_1} \dots T_{\mu_n,\nu_n}0
\end{align}

\begin{theorem}
  For any state $x$, let $p_x$ and $c_x$ be the smallest integers such that
  \begin{align}
    \1_x P_{\mu_1,\nu_1} \dots P_{\mu_{p_x},\nu_{p_x}} = \1_x P_{\mu_1,\nu_1} \dots P_{\mu_{p_x+c_x},\nu_{p_x+c_x}}.
  \end{align}
  Then
  \begin{align}
    v_*(x) = T_{\mu_1,\nu_1}\dots T_{\mu_{p_x},\nu_{p_x}} (T_{\mu_{p_x+1},\nu_{p_x+1}} \dot(T_{\mu_{p_x+1},\nu_{p_x+1}} )^\infty 0.  
  \end{align}
\end{theorem}


\begin{proof}
  Assume MIN uses $\nu_1,\dots,\nu_n$ to play $n$ steps against the optimal policy $\mu_*$ of MAX from $x$. Consider the $n+1$ vertices visited:
  \begin{align}
    x_0=x,~ x_1,~ x_2,~ \dots,~ x_n.
  \end{align}
  Since there are $n$ different vertices, by the pigeonhole principle, there necessarily exists $0 \le p < p+c \le n$ such that $x_p=x_{p+c}$.
  
  Now, assume that against $\mu_*$, MIN uses the strategy $\bar\nu = \nu_1,\dots,\nu_p, (\nu_{p+1} \dots \nu_{p+c})^\infty$
  The trajectory is made of a path followed by a cycle of length $c$ that is repeated infinitely often:
  \begin{align}
    \underbrace{x_0=x,~ x_1,~ x_2,~ \dots, x_{i-1}}_{\mbox{path}},~ \underbrace{x_i,~ x_{i+1},~ \dots,~ x_{j-1}}_{\mbox{cycle}},~ \underbrace{x_i,~ x_{i+1},~ \dots, x_{j-1}}_{\mbox{cycle}}, ~\dots
  \end{align}

  The value of this game satisfies for any $w$,
  \begin{align}
    v_{\mu_*,\bar\nu}(x) - w(x)& = \1_x (T_{\mu_*,\vec\nu_p\vec\nu_c} (T_{\mu_*,\vec\nu_c})^\infty w - w)\\
    & = \1_x  T_{\mu_*,\vec\nu_p\vec\nu_c} 0 + \gamma^{j} \1_{x_i} \sum_{k=0}^{\infty} [(T_{\mu_*,\vec\nu_c})^{k+1} w - T_{\mu_*,\vec\nu_c})^k w] \\
    & = \1_x  T_{\mu_*,\vec\nu_p\vec\nu_c} w + \gamma^{j} \1_{x_i} \sum_{k=0}^{\infty} \gamma^{(j-i)k}(P_{\mu_*,\vec\nu_c})^k (T_{\mu_*,\vec\nu_c} w -  w)  \\
%    & = \1_x  T_{\mu_*,\vec\nu_p\vec\nu_c} w + \gamma^{j} \1_{x_i} (I-\gamma^{j-i} P_{\mu_*,\vec\nu_c})^{-1} (T_{\mu_*,\vec\nu_c} w -  w) \\
    &=\1_x  T_{\mu_*,\vec\nu_p\vec\nu_c} w + \frac{\gamma^{j}}{1-\gamma^{j-i}}\1_{x_i} (T_{\mu_*,\vec\nu_c} w -  w)  \\
    &\le \1_x  \tilde T_{\vec\nu_p\vec\nu_c} w + \frac{\gamma^{j}}{1-\gamma^{j-i}}\1_{x_i} (\tilde T_{\vec\nu_c} w -  w).
  \end{align}
  Taking $w = \tilde T_{\vec\nu_{p'}}v$, we obtain
  \begin{align}
    v_{\mu_*,\bar\nu}(x) - [ \tilde T_{\vec\nu_{p'}}v](x) & \le \1_x  (\tilde T_{\vec\nu_p\vec\nu_c} \tilde T_{\vec\nu_{p'}}v-T_{\vec\nu_{p'}}v) + \frac{\gamma^{j} }{1-\gamma^{j-i}}\1_{x_i} ( \tilde T_{\vec\nu_c} \tilde T_{\vec\nu_{p'}}v - \tilde T_{\vec\nu_{p'}} v )\\
    & = \1_x  (\tilde T_{\vec\nu_p\vec\nu_c\vec\nu_{p'}}v - T_{\vec\nu_{p'}}v)+ \frac{\gamma^{j} }{1-\gamma^{j-i}} \1_{x_i}(\tilde T_{\vec\nu_c\vec\nu_{p'}}v -  \tilde T_{\vec\nu_{p'}} v ) \\
  %  & \le \1_x  (\tilde T_{\vec\nu_p\vec\nu_c} T^{n-i}v - v)+ \frac{\gamma^{j} \1_{x_i}}{1-\gamma^{j-i}} (\tilde T_{\vec\nu_c} T^{n-j}v -  T^{n-j}v ) \\
    & = \1_x (T^nv - T^{n-j}v) + \frac{\gamma^{j}}{1-\gamma^{j-i}}  \1_{x_i} (T^{n-i}v -  T^{n-j}v ) \\
    & \le \1_x (T^nv - v) + \frac{\gamma^{j}}{1-\gamma^{j-i}}  \1_x (T^n v - v) \\
    & \le \frac{\epsilon}{1-\gamma},
  \end{align}
  where we eventually used the facts that $T^nv - v \le \epsilon$, $j \ge 1$ and $j-i \ge 1$.
The result follows by the facts that $v_*(x) = v_{\mu_*,\nu_*}(x) \le v_{\mu_*,\bar\nu}(x)$ and $T^n v \ge T^{n-j}v = \tilde T_{\vec\nu_{p'}}v$.
\end{proof}



 



\bibliographystyle{plain}
\bibliography{biblio}


\end{document}


