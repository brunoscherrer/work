\documentclass{article}
\usepackage{fullpage}
\usepackage[utf8x]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{autonum}
\usepackage{dsfont}
\usepackage{stmaryrd}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}


\title{A polynomial algorithm for the deterministic mean payoff game}

\author{Bruno Scherrer}
\def\R{\mathds R}
\def\1{{\mathds 1}}

\newcommand{\intset}[1]{\llbracket #1 \rrbracket}
\newcommand{\intpart}[1]{\lceil #1 \rceil}

\begin{document}
\maketitle

\begin{abstract}
...
\end{abstract}
  
We consider an infinite-horizon game on a directed graph $(X,E)$ between two players, MAX and MIN. For any vertex $x$, we write $E(x)=\{y;(x,y) \in E\}$ for the set of vertices that can be reached from $x$ by following one edge and we assume $E(x)\neq\emptyset$.  The set of vertices $X=\{1,2,\dots,n\}$ of the graph is partitionned into the sets $X_+$ and $X_-$ of nodes respectively controlled by MAX and MIN. The game starts in some vertex $x_0$. At each time step, the player who controls the current vertex chooses a next vertex by following an edge. So on and so forth, the choices generate an infinitely long trajectory $(x_0,x_1,\dots)$. In the mean payoff game, the goal of MAX is to maximize
\begin{align}
\lim\inf_{T \to \infty} \frac 1 T \sum_{t=0}^{T} r(x_t),
\end{align}
while that of MIN is to minimize
\begin{align}
\lim\sup_{T \to \infty} \frac 1 T \sum_{t=0}^{T} r(x_t).
\end{align}
As a proxy to solve the mean payoff game, our technical developments will mainly consider the $\gamma$-discounted payoff for some $0\le\gamma<1$, where the goal of MAX is to maximize
\begin{align}
(1-\gamma)\sum_{t=0}^{\infty} \gamma^t r(x_t)
\end{align}
while that of MIN is to minimize this quantity.

LITERATURE

\section{Preliminaries}

Let $M$ and $N$ be the set of stationary policies for MAX and MIN:
\begin{align}
  M &= \{ \mu:X_+ \to X ~;~ \forall x\in X_+,~ \mu(x) \in E(x) \}, \\
  N & =\{ \nu:X_- \to X ~;~ \forall x\in X_-,~ \nu(x) \in E(x) \}.
\end{align}

For any policies $\mu \in M$ and $\nu \in N$, let us write $P_{\mu,\nu}$ for the transition matrix induced by $\mu$ and $\nu$:
\begin{align}
  \forall x \in X_+, \forall y \in X, ~~~P_{\mu,\nu}(x,y)=\1_{\mu(x)=y}, \\
  \forall x \in X_-, \forall y \in X, ~~~P_{\mu,\nu}(x,y)=\1_{\nu(x)=y}.
\end{align} 
Seeing the reward $r:X \to {0,1,\dots,R}$ and any function $v:X \to \R$ as vectors of $\R^n$, consider the following Bellman operators
\begin{align}
  T_{\mu,\nu}v & = (1-\gamma)r + \gamma P_{\mu,\nu}v, \\
  T_{\mu}v & = \min_{\nu} T_{\mu,\nu}v, \\
  \tilde T_{\nu}v & = \max_\mu T_{\mu,\nu}v, \\
  T v & =\max_\mu T_{\mu }v = \min_\nu \tilde T_{\nu}v.
\end{align}
that are $\gamma$-contractions with respect to the max-norm $\|\cdot\|$, defined for all $u \in R^n$ as $\|u\|=\max_{x \in X} |u(x)|$.
For any policies $\mu \in M$ and $\nu \in N$, the value $v_{\mu,\nu}(x)$ obtained by following policies $\mu$ and $\nu$ satisfies
\begin{align}
v_{\mu,\nu} = (1-\gamma)\sum_{t=0}^\infty (\gamma P_{\mu,\nu})^t r = (1-\gamma)(I-\gamma P_{\mu,\nu})^{-1}r,
\end{align}
and is the only fixed point of the operator $T_{\mu,\nu}$.
Given any policy $\mu$ for MAX, the minimal value that MIN can obtain
\begin{align}
  v_{\mu} = \min_{\nu} v_{\mu,\nu}
\end{align}
is the fixed point of the operator $T_\mu$, and it is well known that any policy $\nu_+$ for MIN such that $T_{\mu,\nu_+}v_\mu = T_\mu v_\mu = v_\mu$ is optimal.
Symmetrically, given any policy $\nu$ for MIN, the maximal value that MAX can obtain
\begin{align}
  \tilde v_{\mu} = \max_{\nu} v_{\mu,\nu}
\end{align}
is the fixed point of $\tilde T_\nu$, and it is well known that any policy $\mu_+$ for MAX such that $T_{\mu_+,\nu}v_\mu = \tilde T_\nu \tilde v_\nu = \tilde v_\nu$ is optimal.
The optimal value
\begin{align}
  v_* = \max_{\mu} \min_{\nu} v_{\mu,\nu}
\end{align}
is the fixed point of the operator $T$. Let $(\mu_*,\nu_*)$ be any pair of positional strategies such that $T_{\mu_*,\nu_*}v_*=T v_*$. It is well-known that $(\mu_*,\nu_*)$ is optimal.


We shall consider policies that are more complicated than usual stationary policies. 




\section{A local Bellman equation}

The system of equations
\begin{align}
  \forall x,~~ v(x)=[T v](x),
\end{align}
that characterizes the optimal value $v_*$ of the game, is \emph{global} in the sense that it involves the values of \emph{all} the vertices. We shall begin by describing and prove an approximate-optimality equation that has the virtue of being \emph{local} in the sense that it involves only \emph{one} vertex:
\begin{lemma}
  Let $v$ be any value function that satisfies $v \le Tv$.
  If for some $x$, we have
  \begin{align}
    [T^n v](x) - v(x) \le \epsilon,
  \end{align}
  Then 
  \begin{align}
    v_*(x) - [T^n v](x) \le \frac{\epsilon}{1-\gamma}.
  \end{align}
\end{lemma}
\begin{proof}
  First, observe that by the monotonicity of $T$, and since $v \le Tv$, we have
  \begin{align}
    v \le Tv \le T^2 v \le \dots \le T^n v.
  \end{align}
  Let $\vec\nu=(\nu_1,\dots,\nu_n)$ be a policy such that
  \begin{align}
    T^n v = \tilde T_{\vec\nu} v.
  \end{align}
  Assume MIN uses $\vec\nu$ to play $n$ steps against the optimal policy $\mu_*$ of MAX from $x$. Consider the $n+1$ vertices visited:
  \begin{align}
    x_0=x,~ x_1,~ x_2,~ \dots,~ x_n.
  \end{align}
  Since there are $n$ different vertices, by the pigeonhole principle, there necessarily exists $0 \le i < j \le n$ such that $x_i=x_j$. Let $\vec\nu_p=(\nu_1,\dots,\nu_{i-1})$, $\vec\nu_c=(\nu_i,\dots,\nu_{j-1})$ and $\vec\nu_{p'}=(\nu_j,\dots,\nu_n)$ so that $\vec\nu=\vec\nu_p \vec\nu_c \vec\nu_{p'}$.
  
  Now, assume that against $\mu_*$, MIN uses the non-stationary policy $\vec\nu'=\vec\nu_p(\vec\nu_c)^\infty$. The trajectory is made of a path followed by a cycle of length $j-i$ that is repeated infinitely often:
  \begin{align}
    \underbrace{x_0=x,~ x_1,~ x_2,~ \dots, x_{i-1}}_{\mbox{path}},~ \underbrace{x_i,~ x_{i+1},~ \dots,~ x_{j-1}}_{\mbox{cycle}},~ \underbrace{x_i,~ x_{i+1},~ \dots, x_{j-1}}_{\mbox{cycle}}, ~\dots
  \end{align}

  SIMPLIFY!

  The value of this game satisfies for any $w$,
  \begin{align}
    v_{\mu_*,\bar\nu}(x) - w(x)& = \1_x (T_{\mu_*,\vec\nu_p\vec\nu_c} (T_{\mu_*,\vec\nu_c})^\infty w - w)\\
    & = \1_x  T_{\mu_*,\vec\nu_p\vec\nu_c} 0 + \gamma^{j} \1_{x_i} \sum_{k=0}^{\infty} [(T_{\mu_*,\vec\nu_c})^{k+1} w - T_{\mu_*,\vec\nu_c})^k w] \\
    & = \1_x  T_{\mu_*,\vec\nu_p\vec\nu_c} w + \gamma^{j} \1_{x_i} \sum_{k=0}^{\infty} \gamma^{(j-i)k}(P_{\mu_*,\vec\nu_c})^k (T_{\mu_*,\vec\nu_c} w -  w)  \\
%    & = \1_x  T_{\mu_*,\vec\nu_p\vec\nu_c} w + \gamma^{j} \1_{x_i} (I-\gamma^{j-i} P_{\mu_*,\vec\nu_c})^{-1} (T_{\mu_*,\vec\nu_c} w -  w) \\
    &=\1_x  T_{\mu_*,\vec\nu_p\vec\nu_c} w + \frac{\gamma^{j}}{1-\gamma^{j-i}}\1_{x_i} (T_{\mu_*,\vec\nu_c} w -  w)  \\
    &\le \1_x  \tilde T_{\vec\nu_p\vec\nu_c} w + \frac{\gamma^{j}}{1-\gamma^{j-i}}\1_{x_i} (\tilde T_{\vec\nu_c} w -  w).
  \end{align}
  Taking $w = \tilde T_{\vec\nu_{p'}}v$, we obtain
  \begin{align}
    v_{\mu_*,\bar\nu}(x) - [ \tilde T_{\vec\nu_{p'}}v](x) & \le \1_x  (\tilde T_{\vec\nu_p\vec\nu_c} \tilde T_{\vec\nu_{p'}}v-T_{\vec\nu_{p'}}v) + \frac{\gamma^{j} }{1-\gamma^{j-i}}\1_{x_i} ( \tilde T_{\vec\nu_c} \tilde T_{\vec\nu_{p'}}v - \tilde T_{\vec\nu_{p'}} v )\\
    & = \1_x  (\tilde T_{\vec\nu_p\vec\nu_c\vec\nu_{p'}}v - T_{\vec\nu_{p'}}v)+ \frac{\gamma^{j} }{1-\gamma^{j-i}} \1_{x_i}(\tilde T_{\vec\nu_c\vec\nu_{p'}}v -  \tilde T_{\vec\nu_{p'}} v ) \\
  %  & \le \1_x  (\tilde T_{\vec\nu_p\vec\nu_c} T^{n-i}v - v)+ \frac{\gamma^{j} \1_{x_i}}{1-\gamma^{j-i}} (\tilde T_{\vec\nu_c} T^{n-j}v -  T^{n-j}v ) \\
    & = \1_x (T^nv - T^{n-j}v) + \frac{\gamma^{j}}{1-\gamma^{j-i}}  \1_{x_i} (T^{n-i}v -  T^{n-j}v ) \\
    & \le \1_x (T^nv - v) + \frac{\gamma^{j}}{1-\gamma^{j-i}}  \1_x (T^n v - v) \\
    & \le \frac{\epsilon}{1-\gamma},
  \end{align}
  where we eventually used the facts that $T^nv - v \le \epsilon$, $j \ge 1$ and $j-i \ge 1$.
The result follows by the facts that $v_*(x) = v_{\mu_*,\nu_*}(x) \le v_{\mu_*,\bar\nu}(x)$ and $T^n v \ge T^{n-j}v = \tilde T_{\vec\nu_{p'}}v$.
\end{proof}



\section{A Policy Iteration procedure}

Consider the extension of the binary relations $(=,<,\le,>,\ge)$ to $\{0,1\}\times \R$ by using the lexicographic order. For instance,
\begin{align}
  (a,b) < (a',b') ~~\Leftrightarrow~~ a<a' \mbox{ or }(a=a' \mbox{ and }b<b').
\end{align}
Consider also the componentwise substraction:
\begin{align}
  (a,b) - (a',b') = (a-a',b-b').
\end{align}
For any $c \in \R^n$, consider the following operators on $\{0,1\}^n \times \R^n$
\begin{align}
  U_{c,\mu,\nu} (b,v) &= (~ \min(c, P_{\mu,\nu}b),~ T_{\mu,\nu}v ~),\\
  U_{c,\mu} (b,v) & = \min_{\nu} U_{c,\mu,\nu}(b,v), \\
  U_c(b,v) & = \max_{\mu} U_{c,\mu} (b,v).
\end{align}
The primary objective of MAX is to avoid any node $x$ with value $c(x)=0$, while that of MIN is to visit at least one such node. The second objective is the usual 


We first consider a Policy Iteration procedure that takes as parameters a threshold $\rho$ and an initial policy $\mu_0$, and that returns a policy.
  \begin{enumerate}
  \item (Initialization) Set $k=0$, $C=\emptyset$.
  \item (Evaluation) Compute the value $v_k$ when MIN plays optimally against $\mu_k$:
    \begin{align}
      v_k = T_{\mu_k} v_k.
    \end{align}
  \item (n-step advantage) Let $c(x)=\1_{x \not\in C}$ and identify a policy $\vec\mu'$ such that:
    \begin{align}
      (b', v') = (U_c)^n (c,v_k) = U_{c,\vec\mu'} (c,v_k).
    \end{align}
    If for all $x$, $b'(x)=0$, stop and return $\mu_k$.
  \item (Identification of converged states and policy) Update the set
      \begin{align}
        C = \{ x ~;~ b'(x)=0 \mbox{ or }v'(x)-v_k(x) \le (1-\gamma)\rho \},
      \end{align}
    \item (Reduce to a stationary policy) Compute the next stationary policy $\mu_{k+1}$ that is stationary and better than $\vec\mu'$.
  \end{enumerate}

\paragraph{Analysis}
  
  $c(x)$ 

  $c'(x)$ is equal to $1$ if MAX can force to cycle on states that never visit $C$. For these states, we make significant progress.

  
  Consider the situation after step 4 of the algorithm, when after we have computed $C'$ and $\vec\mu'$. Let $\vec\nu'$ be the best policy for MIN when playing against $\vec\mu'$. 
  
  If $x \in X \backslash C'$, that is such that $c'(x)=1$ and $v'(x)-v_k(x) > (1-\gamma)\rho$.
  From $x$, consider the 
  Consider the state $y$ such that $\1_x P_{\vec\mu'\vec\nu'} = \1_y$. We necessarily have $c(y)=1$.

  
  
  \begin{align}
    v_{\vec\mu'}(x) - v_k(x) = \1_x (I-\gamma^n P_{\mu'})^{-1}
  \end{align}

  
  When the procedure stops, we know that for all $\mu$, there exists a policy $\nu$ such that 


  
  \begin{theorem}
    The Policy Iteration procedure stops after at most $n+\frac{n (v_{\mu_*}-v_{\mu_0})}{\rho}$ iterations, the algorithm stops and returns a policy $\mu$ such that
    \begin{align}
      v_*(x) - v_{\vec\mu_x}(x) \le \rho + n(1-\gamma) R 
    \end{align}
  \end{theorem}

  
  Starting from $\rho_0=\frac{W}{2}$, let us choose the sequence of parameters
  \begin{align}
    \rho_{k+1} = \frac{\rho_k+n(1-\gamma) R}{2}
  \end{align}
  so that each call to the procedure lasts at most $2n$ iterations.

  Then after $k$ iterations, we have
  \begin{align}
    v_* - v_{\mu_k} &\le \frac{W}{2^k} + \sum_{i=0}^{k-1} \frac{1}{2^i}(1-\gamma)n R \\
    &\le \frac{W}{2^k} + 2(1-\gamma)n R
  \end{align}
  
  For the mean payoff game, we have
  \begin{align}
    \| g_* - g_{\mu_k} \| & \le 4n(1-\gamma)R + \|v_*-v_{\mu_k}\| \\
    & \le 6n(1-\gamma)R + \frac{W}{2^k}
  \end{align}
  When this is smaller thant $\frac{1}{n^2}$, we are done!
    
 



\bibliographystyle{plain}
\bibliography{biblio}


\end{document}


