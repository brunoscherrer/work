\documentclass{article}
\usepackage{fullpage}
\usepackage[utf8x]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{autonum}
\usepackage{dsfont}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}


\title{A polynomial algorithm for deterministic mean-payoff games}

\author{Bruno Scherrer\footnote{INRIA, bruno.scherrer@inria.fr}}

\def\G{{\cal G}}
\def\H{{\cal H}}
\def\M{{\cal M}}
\def\1{{\mathds 1}}
\def\N{\mathbb N}
\def\R{\mathbb R}
\def\={\stackrel{def}{=}}
\def\Xmax{X_{+}}
\def\Xmin{X_{-}}
\newcommand{\suc}[1]{E(#1)}
\def\stat{\mathop{Stationary}}

\begin{document}
\maketitle

\begin{abstract}
  We describe a polynomial algorithm for solving deterministic mean payoff games. Our algorithm solves a mean payoff game with $n$ vertices and integer edge-costs between $-W$ and $W$ in time ... This in particular implies that a parity game with $n$ vertices and $d$ priorities can be solved in time .... This answers positively the long-standing open problem whether these problems are in $P$.
\end{abstract}

\section*{Introduction}

Consider a mean payoff game played by two players, MAX and MIN, on a graph with $n$ vertices $X=\{1,2,\dots,n\}=\Xmax \sqcup \Xmin$ and directed edges $E$. For any vertex $x$, we write $\suc{x}=\{y;(x,y) \in E\}$ the set of vertices that can be reached from $x$ by following one edge. An integer cost $-R \le r(x) \le R$ is associated to each node $x$. The vertices of $\Xmax$ (resp. $\Xmin$) belong to MAX (resp. MIN). The game starts in some vertex $x_0$. The player who owns the current vertex chooses a next vertex by following an edge. So on and so forth, these choices generate an infinitely long trajectory $(x_0,x_1,\dots)$. The goal of MAX is to maximize
\begin{align}
\lim\inf_{T \to \infty} \frac 1 T \sum_{t=0}^{T}  r(x_t),
\end{align}
while that of MIN is to minimize
\begin{align}
\lim\sup_{T \to \infty} \frac 1 T \sum_{t=0}^{T}  r(x_t).
\end{align}

\section{Preliminaries}

\paragraph{Transition matrix}

For any pair of positional strategies $\mu:\Xmax \to X$ for MAX and $\nu:\Xmin$ for MIN (mappings such that for all $x$, $\mu(x) \in \suc{x}$ and $\nu(x) \in \suc{x}$), let us write $P_{\mu,\nu}$ for the transition matrix: for all $(i,j) \in \{1,2,\dots,n\}^2$, $P_{\mu,\nu}(i,j)$ equals $1$ if and only if $\mu$ and $\nu$ induce a transition $i \to j$ and $0$ else. 


\paragraph{Discounted value}

For any $0<\gamma<1$, let us introduce the following Bellman operator
\begin{align}
  T^{(\gamma)}_{\mu,\nu}v &= r + \gamma P_{\mu,\nu}v,%\\
%  T^{(\gamma)}_{\mu} v &= \min_{\nu} T^{(\gamma)}_{\mu,\nu}v,\\
%  T^{(\gamma)} v  & = \max_{\mu} T^{(\gamma)}_{\mu}v, 
\end{align}
that is a $\gamma$-contraction with respect to the max norm.
The discounted value $v^{(\gamma)}_{\mu,\nu}$ when MAX and MIN respectively use $\mu$ and $\nu$ satisfies
\begin{align}
  v^{(\gamma)}_{\mu,\nu} = \sum_{t=0}^{\infty} (\gamma P_{\mu,\nu})^t r = (I-\gamma P_{\mu,\nu})^{-1} r
\end{align}
and is the fixed point of $T^{(\gamma)}_{\mu,\nu}$.
%If MAX uses the positional policy $\mu$, the optimal discounted value
%\begin{align}
%v^{(\gamma)}_\mu = \min_{\nu} v^{(\gamma)}_{\mu,\nu}
%\end{align}
%that MIN can obtain is the fixed point of 
%$T^{(\gamma)}_\mu$ and any policy $\nu$ that satisfies $T^{(\gamma)}_{\mu} v^{(\gamma)}_\mu = T^{(\gamma)}_{\mu,\nu} v^{(\gamma)}_\mu$. Finally, the optimal value of the discounted game
%\begin{align}
%v^{(\gamma)}_* = \max_{\mu} v^{(\gamma)}_\mu
%\end{align}
%is the fixed point

\paragraph{Gain, bias, Laurent series expansion of the value} 

Write
\begin{align}
P_{\mu,\nu}^* = \lim_{T \to \infty} \frac{1}{T}\sum_{t=0}^{T-1} (P_{\mu,\nu})^t. 
\end{align}
for the Cesaro-limit of $P_{\mu,\nu}$.
The Laurent series expansion \cite[Appendix A]{puterman} tells us that: 
\begin{align}
(I-\gamma P_{\mu,\nu})^{-1} = \frac{P^*_{\mu,\nu}}{1-\gamma} + Q_{\mu,\nu} + O(1-\gamma)
\end{align}
with
\begin{align}
Q_{\mu,\nu} & = (I- P_{\mu,\nu} + P^*_{\mu,\nu})^{-1} (I-\gamma P_{\mu,\nu}^*).
\end{align}

For any policies $\mu \in M$ and $\nu \in N$, let $g_{\mu,\nu}$ and $h_{\mu,\nu}$ be the gain and bias:
\begin{align}
  g_{\mu,\nu} &= P_{\mu,\nu}^* r,\\
  h_{\mu,\nu} &= Q_{\mu,\nu} r.
\end{align}
We deduce that
\begin{align}
  v^{[\gamma)}_{\mu,\nu} = \frac{g_{\mu,\nu}}{1-\gamma} + h_{\mu,\nu} + O(1-\gamma).
\end{align}

\paragraph{Mean payoff operators}

Consider the following operator
\begin{align}
  H^{g}_{\mu,\nu} h & = r-g + P_{\mu,\nu}h.
\end{align}
Given some policies $(\mu,\nu)$, the gain $g_{\mu,\nu}$ and the bias $h_{\mu,\nu}$ are solutions to the following system of linear equations
\begin{align}
  g &= P_{\mu,\nu} g, \\
  h &= H^{g}_{\mu,\nu} h, \\
  w &= h + P_{\mu,\nu} w,
\end{align}
where the extra-variable $w$ ensures that the above system has as a unique solution $g_{\mu,\nu}$ and $h_{\mu,\nu}$ as defined above \cite[Theorem 8.2.6 and Corollary 8.2.9]{puterman}.

Consider the following Bellman operators
\begin{align}
G_{\mu} g & = \min_\nu P_{\mu,\nu} g,  \\
G g &= \max_\mu G_{\mu} g, \\
\G g & = \arg\max_\mu G_{\mu} g, \\
H^{g}_\mu h & = \min_\nu H^{g}_{\mu,\nu} h, \\
\H^g_{\M} h & = \max_{\mu \in \M} H^g_\mu h.
\end{align}

\begin{lemma}
  For any $\mu \in M$, $\nu \in N$, for any $m$, $\vec\mu' \in M^m$ and $\vec\nu' \in N^m$,
  \begin{align}
\frac{g_{\vec\mu',\vec\nu'} - g_{\mu,\nu}}{1-\gamma} + h_{\vec\mu',\vec\nu'} - h_{\mu,\nu} + O(1-\gamma) & = (I-\gamma^m P_{\vec\mu',\vec\nu'})^{-1} \left[ \frac{P_{\vec\mu',\vec\nu'}g_{\mu,\nu}-g_{\mu,\nu}}{1-\gamma} + H^{P_{\vec\mu',\vec\nu'}g_{\mu,\nu}}_{\vec\mu',\vec\nu'} h_{\mu,\nu}-h_{\mu,\nu} + O(1-\gamma) \right].
  \end{align}
\end{lemma}
\begin{proof}
We have for any $\gamma$,
\begin{align}
 v^{(\gamma)}_{\vec\mu',\vec\nu'} - v^{(\gamma)}_{\mu,\nu}    &= (I-\gamma^m P_{\vec\mu',\vec\nu'})^{-1}[ T^{(\gamma)}_{\vec\mu',\vec\nu'}0 + (\gamma^m P_{\vec\mu',\vec\nu'}-I)v^{(\gamma)}_{\mu,\nu} ].
\end{align}
Now observe that
\begin{align}
  & T^{(\gamma)}_{\vec\mu',\vec\nu'}0 + (\gamma^m P_{\vec\mu',\vec\nu'}-I)v^{(\gamma)}_{\mu,\nu} \\
    =~& T^{(\gamma)}_{\vec\mu',\vec\nu'}0 + (\gamma^m P_{\vec\mu',\vec\nu'}-I)\left( \frac{g_{\mu,\nu}}{1-\gamma} + h_{\mu,\nu} + O(1-\gamma) \right) \\
  =~ & T^{(\gamma)}_{\vec\mu',\vec\nu'}0 + [P_{\vec\mu',\vec\nu'}-I-(1-\gamma^m)P_{\vec\mu',\vec\nu'})] \left( \frac{g_{\mu,\nu}}{1-\gamma} + h_{\mu,\nu} + O(1-\gamma) \right) \\
  =~ & \frac{P_{\vec\mu',\vec\nu'}g_{\mu,\nu}-g_{\mu,\nu}}{1-\gamma} + T^{(1)}_{\vec\mu',\vec\nu'}0  + P_{\vec\mu',\vec\nu'}(h_{\mu,\nu}- m g_{\mu,\nu}) -h_{\mu,\nu} + O(1-\gamma) \\
  =~ & \frac{P_{\vec\mu',\vec\nu'}g_{\mu,\nu}-g_{\mu,\nu}}{1-\gamma} + T^{(1)}_{\vec\mu',\vec\nu'}(h_{\mu,\nu}-n g_{\mu,\nu}) -h_{\mu,\nu} + O(1-\gamma) \\
  =~ & \frac{P_{\vec\mu',\vec\nu'}g_{\mu,\nu}-g_{\mu,\nu}}{1-\gamma} + H^{P_{\vec\mu',\vec\nu'}g_{\mu,\nu}}_{\vec\mu',\vec\nu'} h_{\mu,\nu}-h_{\mu,\nu} + O(1-\gamma).
\end{align}
\end{proof}

Computation of a stationary policy that is better than a non-stationary policy.
Compute the values $w_k^{(n)},\dots,w_k^{(1)}$ in the 1-player problems for MIN where $MAX$ uses the $n$-periodic strategies $\sigma_k^{(n)} = (\mu_k^{(n)},\dots,\mu_k^{(1)})$, $\sigma_k^{(n-1)} = (\mu_k^{(n-1)},\dots,\mu_k^{(1)},\mu_k^{(n)})$, $\dots$, $\sigma_k^{(1)}=(\mu_k^{(1)},\mu_k^{(n)},\dots,\mu_k^{(2)})$:
\begin{align}
  w_k^{(n)} &= T_{\mu_k^{(n)}} \dots T_{\mu_k^{(1)}} w_k^{(n)}, \\
  w_k^{(n-1)} &= T_{\mu_k^{(n-1)}} \dots T_{\mu_k^{(1)}} T_{\mu_k^{(n)}} w_k^{(n-1)}, \\
  \vdots ~~~~ & ~~~~~~~~~~~~~~~~~~~ \vdots \\
  w_k^{(1)} &= T_{\mu_k^{(1)}} T_{\mu_k^{(n)}} \dots T_{\mu_k^{(2)}}  w_k^{(1)}.
\end{align}
Compute the pointwise maximum $w_k=\max_{i}w_k{(i)}$, and 
identify a policy $\mu_{k+1}$ that satisfies:
\begin{align}
  T_{\mu_k} w_k = T w_k
\end{align}


\section{A Policy Iteration algorithm}
\label{algo}

We consider the following iterative algorithm:
\begin{enumerate}
\item (Initialization) Set $k=0$ and take an arbitrary policy $\mu_0 \in M$ for MAX.
\item (Evaluation) Compute the optimal gain $g_k$ and bias $h_k$ for MIN in the 1-player problem where MAX uses $\mu_k$ by solving the system:
  \begin{align}
    g_k & = G_{\mu_k} g_k, \\
    h_k & = H_{\mu_k}^{g_k}h_k = H^{g_k}_{\mu_k,\nu_k}h_k, \\
    w_k & = h_k + P_{\mu_k,\nu_k} w_k.
  \end{align}
\item (Optimisation of the $n$-step policy) Setting $\tilde g_{k,n}=g_k$, compute for $i=n-1,n-2,\dots,0$
  \begin{align}
    \tilde g_{k,i} & = G \tilde g_{k,i+1}, \\
    \M_{k,i} & = \G \tilde g_{k,i+1}.
  \end{align}
  Then, compute for $i=n-1,n-2,\dots,0$ and identify a sequence of policies $(\tilde \mu_{k,n-1},\tilde \mu_{k,n-2},\dots,\tilde \mu_{k,0}) \in \M_{k,n-1} \times \M_{k,n-2} \times \dots \times \M_{k,0}$ such that
  \begin{align}
    \tilde h_{k,i} & = \H_{\M_{k,i}}^{\tilde g_{k,0}} \tilde h_{k,i+1} = H_{\tilde \mu_{k,i}}^{\tilde g_{k,0}} \tilde h_{k,i+1}. 
  \end{align}
  \item (Identification of nodes with optimal gain) Compute the set
    \begin{align}
      Z_k = \{ x \in X ~;~ (\tilde g_{k,0}(x),\tilde h_{k,0}(x))=(g_k(x),h_k(x)) \}.
    \end{align}
    If $Z_k \neq \emptyset$: 1) remove the nodes of the MIN-attractor $A_k$ set of $Z_k$ from the game (along with the transitions that go to $A_k$). 2) If the game still has nodes,  increment $k$ by $1$ and go to step 2 (otherwise stop)
  \item (Computation of the next stationary policy) Set
    \begin{align}
      \mu_{k+1} = \stat(\tilde \mu_{k,0},\tilde \mu_{k,1},\dots,\tilde \mu_{k,n-1}).
    \end{align}
    Increment $k$ by $1$ and go to step 2.
\end{enumerate}

\section{A scaling approach}
\label{scaling}

\section{Application to the Mean Payoff game}
\label{mpg}

\section{Conclusion}

We have shown that the problem ``Mean Payoff Game'' is in $P$. To our knowledge, this problem was only previously known to be in $NP \cap co-NP$ \cite{zwick}.

It was shown in \cite{puri} that any parity game (a game that is central to $\mu$-calculus model checking) on a graph with $n$ vertices and $d$ priorities can be reduced to a mean payoff game on the same graph with edge costs bounded in absolute value by $W=n^d$. As a consequence, Theorem~\ref{thmpg} implies that:
\begin{theorem}
A parity game with $n$ vertices and $d$ priorities can be solved in time ...
\end{theorem}
Though ``Parity Game'' was long thought to be in $NP \cap co-NP$ and has recently be shown to be quasi-polynomial \cite{calude}, it is in fact in $P$.



%\paragraph{Acknowledgements} 

\bibliographystyle{plain}
\bibliography{biblio.bib} 

\end{document}
