\documentclass{article}
\usepackage{fullpage}
\usepackage[utf8x]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{autonum}
\usepackage{dsfont}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}


\title{A polynomial algorithm for deterministic mean-payoff games}

\author{Bruno Scherrer\footnote{INRIA, bruno.scherrer@inria.fr}}


\def\1{{\mathds 1}}
\def\N{\mathbb N}
\def\R{\mathbb R}
\def\={\stackrel{def}{=}}
\def\Xmax{X_{+}}
\def\Xmin{X_{-}}
\newcommand{\suc}[1]{E(#1)}


\begin{document}
\maketitle

\begin{abstract}
  We describe a polynomial algorithm for solving deterministic mean payoff games. Our algorithm solves a mean payoff game with $n$ vertices and integer edge-costs between $-W$ and $W$ in time ... This in particular implies that a parity game with $n$ vertices and $d$ priorities can be solved in time .... This answers positively the long-standing open problem whether these problems are in $P$.
\end{abstract}

\section{Preliminaries}

Consider a mean payoff game played by two players, MAX and MIN, on a graph with $n$ vertices $X=\{1,2,\dots,n\}=\Xmax \sqcup \Xmin$ and directed edges $E$. For any vertex $x$, we write $\suc{x}=\{y;(x,y) \in E\}$ the set of vertices that can be reached from $x$ by following one edge. An integer cost $-R \le r(x) \le R$ is associated to each node $x$. The vertices of $\Xmax$ (resp. $\Xmin$) belong to MAX (resp. MIN). The game starts in some vertex $x_0$. The player who owns the current vertex chooses a next vertex by following an edge. So on and so forth, these choices generate an infinitely long trajectory $(x_0,x_1,\dots)$. The goal of MAX is to maximize
\begin{align}
\lim\inf_{T \to \infty} \frac 1 T \sum_{t=0}^{T}  r(x_t),
\end{align}
while that of MIN is to minimize
\begin{align}
\lim\sup_{T \to \infty} \frac 1 T \sum_{t=0}^{T}  r(x_t).
\end{align}


\paragraph{Transition matrix}

For any pair of positional strategies $\mu:\Xmax \to X$ for MAX and $\nu:\Xmin$ for MIN (mappings such that for all $x$, $\mu(x) \in \suc(x)$ and $\nu(x) \in \suc(x)$), let us write $P_{\mu,\nu}$ for the transition matrix: for all $(i,j) \in \{1,2,\dots,n\}^2$, $P_{\mu,\nu}(i,j)$ equals $1$ if and only if $\mu$ and $\nu$ induce a transition $i \to j$ and $0$ else. 


\paragraph{Discounted value}

For any $0<\gamma<1$, let us introduce the following Bellman operator
\begin{align}
  T^{(\gamma)}_{\mu,\nu}v &= r + \gamma P_{\mu,\nu}v,%\\
%  T^{(\gamma)}_{\mu} v &= \min_{\nu} T^{(\gamma)}_{\mu,\nu}v,\\
%  T^{(\gamma)} v  & = \max_{\mu} T^{(\gamma)}_{\mu}v, 
\end{align}
that is a $\gamma$-contraction with respect to the max norm.
The discounted value $v^{(\gamma)}_{\mu,\nu}$ when MAX and MIN respectively use $\mu$ and $\nu$ satisfies
\begin{align}
  v^{(\gamma)}_{\mu,\nu} = \sum_{t=0}^{\infty} (\gamma P_{\mu,\nu})^t r = (I-\gamma P_{\mu,\nu})^{-1} r
\end{align}
and is the fixed point of $T^{(\gamma)}_{\mu,\nu}$.
%If MAX uses the positional policy $\mu$, the optimal discounted value
%\begin{align}
%v^{(\gamma)}_\mu = \min_{\nu} v^{(\gamma)}_{\mu,\nu}
%\end{align}
%that MIN can obtain is the fixed point of 
%$T^{(\gamma)}_\mu$ and any policy $\nu$ that satisfies $T^{(\gamma)}_{\mu} v^{(\gamma)}_\mu = T^{(\gamma)}_{\mu,\nu} v^{(\gamma)}_\mu$. Finally, the optimal value of the discounted game
%\begin{align}
%v^{(\gamma)}_* = \max_{\mu} v^{(\gamma)}_\mu
%\end{align}
%is the fixed point

\paragraph{Gain, bias, Laurent series expansion of the value} 
Write
\begin{align}
P_{\mu,\nu}^* = \lim_{T \to \infty} \frac{1}{T}\sum_{t=0}^{T-1} (P_{\mu,\nu})^t. 
\end{align}
for the Cesaro-limit of $P_{\mu,\nu}$.
For any function $g(\cdot)$ of parameter $\gamma$, we shall write
\begin{align}
  g(\gamma)=f_\gamma [a,b]
\end{align}
when $g$ admits a development when $\gamma \uparrow 1$ of the form :
\begin{align}
  g(\gamma)=\frac{a}{1-\gamma}+b+O(1-\gamma).
\end{align}
The Laurent series expansion \cite[Appendix A]{puterman} tells us that: 
\begin{align}
(I-\gamma P_{\mu,\nu})^{-1} = f_\gamma ~(~ P^*_{\mu,\nu}~,~ (I- P_{\mu,\nu} + P^*_{\mu,\nu})^{-1} (I-\gamma P_{\mu,\nu}^*) ].
\end{align}
We deduce that
\begin{align}
  v^{(\gamma)}_{\mu,\nu} = f_\gamma ~[~ g_{\mu,\nu} ~,~ h_{\mu,\nu} ~],
\end{align}
where $g_{\mu,\nu}$ and $h_{\mu,\nu}$ are the gain and the bias defined as
\begin{align}
  g_{\mu,\nu} &= P_{\mu,\nu}^* r,\\
  h_{\mu,\nu} &= [ I-(P_{\mu,\nu}-P^*_{\mu,\nu})]^{-1} (I-P_*)r.
\end{align}
For any $(g,h) \in \R^2$, consider the following Bellman operators
\begin{align}
  T_{\mu,\nu} (g, h) & = (~ P_{\mu,\nu}g,~ r + P_{\mu,\nu}(h-g) ~).
\end{align}
Given some policies $(\mu,\nu)$, the gain $g_{\mu,\nu}$ and the bias $h_{\mu,\nu}$ are solutions to the following system of linear equations
\begin{align}
  (g,h)&=T_{\mu,\nu}(g,h), \\
  w &= h + P_{\mu,\nu} w,
\end{align}
where the extra-variable $w$ ensures that the above system has a unique solution \cite[Theorem 8.2.6 and Corollary 8.2.9]{puterman}.


\paragraph{Order relation and Optimality Bellman operator}
Consider the lexicographic order relation $\prec$ on $\R^2$:
\begin{align}
  (g,h) \prec (g',h') ~~\Leftrightarrow~~ g < g' \mbox{ or }(g=g' \mbox{ and }h < h')
\end{align}
Consider the following Bellman operators
\begin{align}
T_{\mu} (g,h) & = \min_\nu T_{\mu,\nu} (g,h),  \\
T (g,h) &= \max_\mu T_{\mu} (g,h),
\end{align}
where the max and min operators are based on the order relation $\prec$.

\paragraph{Policy Iteration}

The standard Policy Iteration for mean payoff games is based on the following observations:
\begin{lemma}
  Let $\mu$ be some policy for MAX. Let $\nu$ be an optimal counter policy for MIN and $v_{\mu,\nu}=(g_{\mu,\nu},h_{\mu,\nu})$ be the value (gain and bias) of the resulting game. Let $\bar\mu$ be any policy that satisfies $T_{\bar\mu}v=T v$. If for some $x$, $v_{\mu,\nu}(x) \prec T_{\bar\mu}v_{\mu,\nu}(x)$, then $(g_{\mu,\nu},h_{\mu,\nu}) \prec (g_{\bar\mu,\bar\nu},h_{\bar\mu,\bar\nu})$ ; otherwise, $\mu$ is an optimal policy.
\end{lemma}

For the sake of completeness we give a proof.
\begin{proof}
  For any policies $(\mu',\nu')$,
  \begin{align}
    v^{(\gamma)}_{\mu',\nu'} - v^{(\gamma)}_{\mu,\nu} &= (I-\gamma P_{\mu',\nu'})^{-1}[ r + (\gamma P_{\mu',\nu'}-I)v^{(\gamma)}_{\mu,\nu} ].
  \end{align}
  Now observe that
  \begin{align}
     & r + (\gamma P_{\mu',\nu'}-I)\left( \frac{g_{\mu,\nu}}{1-\gamma} + h_{\mu,\nu} + O(1-\gamma) \right) \\
     =~ &r + [P_{\mu',\nu'}-I-(1-\gamma)P_{\mu',\nu'})]\left( \frac{g_{\mu,\nu}}{1-\gamma} + h_{\mu,\nu} + O(1-\gamma) \right) \\
     =~ & \frac{P_{\mu',\nu'}g_{\mu,\nu}-g_{\mu,\nu}}{1-\gamma} + r + P_{\mu',\nu'}(h_{\mu,\nu}-g_{\mu,\nu}) -h_{\mu,\nu} + O(1-\gamma) \\
     =~ & \frac{P_{\mu',\nu'}g_{\mu,\nu}-P_{\mu,\nu}g_{\mu,\nu}}{1-\gamma} + P_{\mu',\nu'}(h_{\mu,\nu}-g_{\mu,\nu}) -P_{\mu,\nu} (h_{\mu,\nu}-g_{\mu,\nu}) + O(1-\gamma)
  \end{align}
\end{proof}
By taking $(\mu',\nu')=(\bar\mu,\bar\nu)$, we get

  


    
\section{A non-stationary Policy Iteration algorithm}
\label{algo}

We consider the following iterative algorithm:
\begin{enumerate}
\item (Initialization) Set $k=0$ and take an arbitrary policy $\mu_0$ for MAX.
\item (Evaluation) Compute the value $v_k$ and an optimal counter-policy $\nu_k$ of MIN in the 1-player problem  where MAX uses $\mu_k$:
  \begin{align}
    v_k = T v_k = T_{\mu_k} v_k = T_{\mu_k,\nu_k} v_k
  \end{align}
\item (Computation of the $n$-step advantage) Compute the advantage $\delta_n$ and a pair of $n$-horizon strategies $(\mu_k^{(n)},\dots,\mu_k^{(1)})$ and $(\nu_k^{(n)},\dots,\nu_k^{(1)})$ such that:
  \begin{align}
    \delta_k = T^n v_k - v_k = T_{\mu_k^{(n)},\nu_k^{(n)}} T_{\mu_k^{(n-1)},\nu_k^{(n-1)}} \dots  T_{\mu_k^{(1)},\nu_k^{(1)}}v_k - v_k.
  \end{align}
\item (Identification of converged nodes) Compute the set
  \begin{align}
    Z_k = \{ x \in X ~;~ \delta_k(x)=0 \}.
  \end{align}
  If $Z_k \neq \emptyset$: 1) remove the nodes of the MIN-attractor $A_k$ set of $Z_k$ from the game (along with the transitions that go to $A_k$).  If the game still has nodes, increment $k$ by $1$ and go to step 2 (otherwise stop). 
\item (Computation of a stationary policy) Compute the values $w_k^{(n)},\dots,w_k^{(1)}$ in the 1-player problems for MIN where $MAX$ uses the $n$-periodic strategies $\sigma_k^{(n)} = (\mu_k^{(n)},\dots,\mu_k^{(1)})$, $\sigma_k^{(n-1)} = (\mu_k^{(n-1)},\dots,\mu_k^{(1)},\mu_k^{(n)})$, $\dots$, $\sigma_k^{(1)}=(\mu_k^{(1)},\mu_k^{(n)},\dots,\mu_k^{(2)})$:
  \begin{align}
    w_k^{(n)} &= T_{\mu_k^{(n)}} \dots T_{\mu_k^{(1)}} w_k^{(n)}, \\
    w_k^{(n-1)} &= T_{\mu_k^{(n-1)}} \dots T_{\mu_k^{(1)}} T_{\mu_k^{(n)}} w_k^{(n-1)}, \\
    \vdots ~~~~ & ~~~~~~~~~~~~~~~~~~~ \vdots \\
    w_k^{(1)} &= T_{\mu_k^{(1)}} T_{\mu_k^{(n)}} \dots T_{\mu_k^{(2)}}  w_k^{(1)}.
  \end{align}
  Compute the pointwise maximum $w_k=\max_{i}w_k{(i)}$, and 
  identify a policy $\mu_{k+1}$ that satisfies:
  \begin{align}
    T_{\mu_k} w_k = T w_k
  \end{align}
  Increment $k$ by $1$ and go to step 2.
\end{enumerate}

\section{A scaling approach}
\label{scaling}

\section{Application to the Mean Payoff game}
\label{mpg}

\section{Conclusion}

We have shown that the problem ``Mean Payoff Game'' is in $P$. To our knowledge, this problem was only previously known to be in $NP \cap co-NP$ \cite{zwick}.

It was shown in \cite{puri} that any parity game (a game that is central to $\mu$-calculus model checking) on a graph with $n$ vertices and $d$ priorities can be reduced to a mean payoff game on the same graph with edge costs bounded in absolute value by $W=n^d$. As a consequence, Theorem~\ref{thmpg} implies that:
\begin{theorem}
A parity game with $n$ vertices and $d$ priorities can be solved in time ...
\end{theorem}
Though ``Parity Game'' was long thought to be in $NP \cap co-NP$ and has recently be shown to be quasi-polynomial \cite{calude}, it is in fact in $P$.



%\paragraph{Acknowledgements} 

\bibliographystyle{plain}
\bibliography{biblio.bib} 

\end{document}
